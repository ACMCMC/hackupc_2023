{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "APPLIANCES_LIST = ['baseboard radiator', 'central ac', 'dishwasher', 'double oven', 'microwave', 'oven', 'radiator', 'range hood', 'range oven', 'separate washer  dryer', 'stacked washer  dryer', 'stovetop', 'refrigerator', 'tv', 'wall mounted ac', 'washer  dryer', 'water heater']\n",
    "\n",
    "ROOMS_LIST = ['2D floor plan', '3D floor plan', 'balcony', 'bathroom', 'cellar', 'details', 'dining room', 'documents', 'empty room', 'energy certificate', 'garden', 'gym', 'hall-corridor', 'kitchen', 'laundry room', 'living-dining room', 'living room', 'map location', 'mountain view', 'non related', 'office', 'outdoor building', 'outdoor house', 'parking', 'pool', 'reception-lobby', 'room-bedroom', 'stairs', 'storage pantry', 'terrace', 'walk in closet', 'water view']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e32db74749b94497a3af35e05082bceb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-xl\")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-xl\")\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"Q: What three home appliances do I need in order to make a coffee? \"\n",
    "PROMPT = \"Suggest at least five related search terms to \\\"Pizza\\\".\"\n",
    "\n",
    "input_ids = tokenizer.encode(PROMPT, return_tensors=\"pt\")\n",
    "if torch.cuda.is_available():\n",
    "    input_ids = input_ids.to('cuda')\n",
    "\n",
    "outputs = model.generate(input_ids, max_length=1000, do_sample=True, top_p=0.95, top_k=60)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloomz-7b1-mt\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloomz-7b1-mt\")\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    pass\n",
    "    #model = model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: In which of the following rooms can I bake a cake?\n",
      "- 2D floor plan\n",
      "- 3D floor plan\n",
      "- balcony\n",
      "- bathroom\n",
      "- cellar\n",
      "- details\n",
      "- dining room\n",
      "- documents\n",
      "- empty room\n",
      "- energy certificate\n",
      "- garden\n",
      "- gym\n",
      "- hall-corridor\n",
      "- kitchen\n",
      "- laundry room\n",
      "- living-dining room\n",
      "- living room\n",
      "- map location\n",
      "- mountain view\n",
      "- non related\n",
      "- office\n",
      "- outdoor building\n",
      "- outdoor house\n",
      "- parking\n",
      "- pool\n",
      "- reception-lobby\n",
      "- room-bedroom\n",
      "- stairs\n",
      "- storage pantry\n",
      "- terrace\n",
      "- walk in closet\n",
      "- water view\n",
      "\n",
      "Answer: \n",
      "<pad> kitchen</s>\n"
     ]
    }
   ],
   "source": [
    "joined_list = '\\n- '.join(ROOMS_LIST)\n",
    "PROMPT = f\"\"\"Question: In which of the following rooms can I bake a cake?\n",
    "- {joined_list}\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "print(PROMPT)\n",
    "\n",
    "# Set the random seed manually for reproducibility\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# The model is a decoder-only model. Do not pad.\n",
    "input_ids = tokenizer.encode(PROMPT, return_tensors=\"pt\", pad_to_max_length=False)\n",
    "if torch.cuda.is_available():\n",
    "    input_ids = input_ids.to('cuda')\n",
    "\n",
    "outputs = model.generate(input_ids, max_length=10, do_sample=True, top_p=0.90, top_k=80)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which of the following appliances are used to bake a cake? Multiple answers are possible.\n",
      "- baseboard radiator\n",
      "- central ac\n",
      "- dishwasher\n",
      "- double oven\n",
      "- microwave\n",
      "- oven\n",
      "- radiator\n",
      "- range hood\n",
      "- range oven\n",
      "- separate washer  dryer\n",
      "- stacked washer  dryer\n",
      "- stovetop\n",
      "- refrigerator\n",
      "- tv\n",
      "- wall mounted ac\n",
      "- washer  dryer\n",
      "- water heater\n",
      "\n",
      "Answer:\n",
      "- \n",
      "<pad> oven</s>\n"
     ]
    }
   ],
   "source": [
    "joined_list = '\\n- '.join(APPLIANCES_LIST)\n",
    "PROMPT = f\"\"\"Which of the following appliances are used to bake a cake? Multiple answers are possible.\n",
    "- {joined_list}\n",
    "\n",
    "Answer:\n",
    "- \"\"\"\n",
    "\n",
    "print(PROMPT)\n",
    "\n",
    "# Set the random seed manually for reproducibility\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# The model is a decoder-only model. Do not pad.\n",
    "input_ids = tokenizer.encode(PROMPT, return_tensors=\"pt\", pad_to_max_length=False)\n",
    "if torch.cuda.is_available():\n",
    "    input_ids = input_ids.to('cuda')\n",
    "\n",
    "outputs = model.generate(input_ids, max_length=1000, do_sample=True, top_p=0.95, top_k=60)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"\"\"For each example, write a list of 3 home appliances that you need to make it.\n",
    "## Example 1: Pizza\n",
    "- Oven\n",
    "- Fridge\n",
    "- Knife\n",
    "## Example 2: Coffee\n",
    "- Coffee machine\n",
    "- Cup\n",
    "- Spoon\n",
    "## Example 3: \"\"\"\n",
    "\n",
    "item = \"Cake\"\n",
    "PROMPT += f\"{item}\\n\"\n",
    "\n",
    "input_ids = tokenizer.encode(PROMPT, return_tensors=\"pt\")\n",
    "if torch.cuda.is_available():\n",
    "    input_ids = input_ids.to('cuda')\n",
    "\n",
    "outputs = model.generate(input_ids, max_length=1000, do_sample=True, top_p=0.95, top_k=60)\n",
    "\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc99c62a9f184d55a1a0a2abc6d453f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/mt0-xl\")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"bigscience/mt0-xl\")\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloomz-3b\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloomz-3b\")\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the info on the CUDA available memory. Print the free memory, and the total memory.\n",
    "print(torch.cuda.memory_summary(device=None, abbreviated=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hugging-face-tests",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
